---
title: "LDA Modelling with 2017"
author: "Nikky Xiong"
date: "March 27, 2020"
output: html_document
---

```{r global options, warning=FALSE, message=FALSE, cache=FALSE}
set.seed(1:7)
library(knitr)
opts_chunk$set(warning = F,message = F,error = F)
library(readr)
library(dplyr)
library(tidytext)
library(lubridate)
library(tm)
library(topicmodels)
library(ggplot2)
library(SnowballC)

df <- readr::read_csv('complaints.csv') %>% 
    filter(`Consumer complaint narrative` != 'NA') %>% 
    filter(`Company response to consumer` == 'Closed with monetary relief') %>%
    mutate(year = as.integer(substr(`Date received`, 
                                  start = 1, stop = 4))) %>%
    mutate(month = as.integer(substr(`Date received`, 
                                   start = 6, stop = 7))) %>%
    mutate(day = as.integer(substr(`Date received`, start =9 , stop = 10))) 
df$`Date received` <- ymd(df$`Date received`) 
df <- df %>% filter(year == '2017')
```

The dataframe contains complaints with narrative made in 2019 and were closed with monetary relief.

```{r}
narrative <- df %>% 
    unnest_tokens(word, `Consumer complaint narrative`) %>%
    anti_join(stop_words)
narrative <- narrative %>% 
    filter(!(word %in% c("xxxx", "xx")))
narrative %>% 
    count(word, sort = TRUE) %>%
    head(25)
```

Write a function that takes a text column from a data frame and returns a plot of the most informative words for a given number of topics.

```{r}
# function to get & plot the most informative terms by a specificed number
# of topics, using LDA
top_terms_by_topic_LDA <- function(input_text, # should be a columm from a dataframe
                                   plot = T, # return a plot? TRUE by defult
                                   number_of_topics = 5) # number of topics (5 by default)
{    
    # create a corpus (type of object expected by tm) and document term matrix
    Corpus <- Corpus(VectorSource(input_text)) # make a corpus object
    DTM <- DocumentTermMatrix(Corpus) # get the count of words/document

    # remove any empty rows in our document term matrix (if there are any 
    # we'll get an error when we try to run our LDA)
    unique_indexes <- unique(DTM$i) # get the index of each unique value
    DTM <- DTM[unique_indexes,] # get a subset of only those indexes
       # preform LDA & get the words/topic in a tidy text format
    lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
    topics <- tidy(lda, matrix = "beta")

    # get the top ten terms for each topic
    top_terms <- topics  %>% # take the topics data frame and..
      group_by(topic) %>% # treat each topic as a different group
      top_n(10, beta) %>% # get the top 10 most informative words
      ungroup() %>% # ungroup
      arrange(topic, -beta) # arrange words in descending informativeness

    # if the user asks for a plot (TRUE by default)
    if(plot == T){
        # plot the top ten terms for each topic in order
        top_terms %>% # take the top terms
          mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
          ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
          geom_col(show.legend = FALSE) + # as a bar plot
          facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
          labs(x = NULL, y = "Beta") + # no x label, change y label 
          coord_flip() # turn bars sideways
    }else{   # if the user does not request a plot
        # return a list of sorted terms instead
        return(top_terms)
    }
}
```

```{r}
reviewsCorpus <- Corpus(VectorSource(df$`Consumer complaint narrative`)) 
reviewsDTM <- DocumentTermMatrix(reviewsCorpus)

# convert the document term matrix to a tidytext corpus
reviewsDTM_tidy <- tidy(reviewsDTM)

# I'm going to add my own custom stop words that I don't think will be very informative in consumer complaints
custom_stop_words <- tibble(word = c("xxxx,", "xx,",'(xx/xx/xxxx),', 'told,'))

# remove stopwords
reviewsDTM_tidy_cleaned <- reviewsDTM_tidy %>% # take our tidy dtm and...
    anti_join(stop_words, by = c("term" = "word")) %>% # remove English stopwords and...
    anti_join(custom_stop_words, by = c("term" = "word")) # remove my custom stopwords
# reconstruct cleaned documents (so that each word shows up the correct number of times)
cleaned_documents <- reviewsDTM_tidy_cleaned %>%
    group_by(document) %>% 
    mutate(terms = toString(rep(term, count))) %>%
    select(document, terms) %>%
    unique()

# check out what the cleaned documents look like (should just be a bunch of content words)
# in alphabetic order
head(cleaned_documents)
```

```{r}
top_terms_by_topic_LDA(cleaned_documents$terms, number_of_topics = 5)
```

Run again after stemming our data.

```{r}
# stem the words (e.g. convert each word to its stem, where applicable)
reviewsDTM_tidy_cleaned <- reviewsDTM_tidy_cleaned %>% 
    mutate(stem = wordStem(term))

# reconstruct our documents
cleaned_documents <- reviewsDTM_tidy_cleaned %>%
    group_by(document) %>% 
    mutate(terms = toString(rep(stem, count))) %>%
    select(document, terms) %>%
    unique()

# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents$terms, number_of_topics = 5)
```

